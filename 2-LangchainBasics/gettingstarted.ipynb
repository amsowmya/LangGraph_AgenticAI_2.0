{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613585dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c708ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "## Langsmith Tracking and Tracing\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f14ba07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! ðŸ‘‹ How can I help you today? ðŸ˜Š\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 11, 'total_tokens': 25, 'completion_time': 0.025454545, 'prompt_time': 0.001940292, 'queue_time': 0.24834859799999998, 'total_time': 0.027394837}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--c13391e2-eef5-410a-9c6e-28327f4e8726-0', usage_metadata={'input_tokens': 11, 'output_tokens': 14, 'total_tokens': 25})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(groq_api_key=groq_api_key, model=\"gemma2-9b-it\")\n",
    "\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ba6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model = ChatOpenAI(model=\"o1-mini\")\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "409e6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an AI engineer specialized in GenAI\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33acc786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an AI engineer specialized in GenAI'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "531be9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = ({\"input\":RunnablePassthrough()}\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser()\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "baad8b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI engineer specializing in GenAI, I can tell you that **GenAI stands for Generative Artificial Intelligence**.  \\n\\nIt's a fascinating and rapidly evolving field of artificial intelligence focused on creating new content. This content can take many forms, including:\\n\\n* **Text:** Stories, poems, articles, code, dialogue, scripts\\n* **Images:** Photos, artwork, illustrations, 3D models\\n* **Audio:** Music, sound effects, speech synthesis\\n* **Video:** Animations, short films\\n\\n**How GenAI Works:**\\n\\nAt its core, GenAI relies on powerful deep learning algorithms, particularly a type called **generative models**. These models are trained on massive datasets of existing content.  Through this training, they learn the underlying patterns, structures, and relationships within the data. \\n\\nOnce trained, these models can generate new content that mimics the style and characteristics of the data they learned from. Think of it like teaching a child to write by showing them thousands of books â€“ they'll eventually learn the rules of grammar and storytelling and be able to create their own stories.\\n\\n**Examples of GenAI in Action:**\\n\\n* **ChatGPT:**  A text-based chatbot that can engage in natural-sounding conversations and generate human-quality text.\\n* **DALL-E 2:**  An image generation model that can create stunning visuals from textual descriptions.\\n* **Jukebox:**  A model that can generate music in various styles and genres.\\n\\n**The Impact of GenAI:**\\n\\nGenAI has the potential to revolutionize many industries, including:\\n\\n* **Creative Industries:** Writing, art, music, and filmmaking.\\n* **Marketing and Advertising:** Creating personalized content and campaigns.\\n* **Education:**  Developing interactive learning experiences and personalized tutoring.\\n* **Research:**  Generating hypotheses and exploring new ideas.\\n\\n**Ethical Considerations:**\\n\\nWhile GenAI offers incredible opportunities, it also raises important ethical considerations, such as:\\n\\n* **Bias:** GenAI models can perpetuate biases present in the training data.\\n* **Misinformation:**  The ability to generate realistic fake content can be misused for malicious purposes.\\n* **Copyright and Intellectual Property:**  Questions arise about ownership and copyright of AI-generated content.\\n\\n\\nIt's a field full of exciting possibilities and challenges, and I'm thrilled to be part of it!\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "799a7eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  input: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an AI engineer specialized in GenAI'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0223897d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an AI engineer specialized in GenAI'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002245DC76D50>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002245DC77390>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72c9d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": \"what is genai\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf1fb8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI engineer specializing in GenAI, I can tell you that **GenAI** stands for **Generative Artificial Intelligence**. \n",
      "\n",
      "Think of it this way:\n",
      "\n",
      "* **Artificial Intelligence (AI):**  A broad field where computers learn to perform tasks that typically require human intelligence, like problem-solving, decision-making, and understanding language.\n",
      "\n",
      "* **Generative:**  Means \"creating something new\".\n",
      "\n",
      "So, **GenAI** is a type of AI that focuses on creating new content. This content can be:\n",
      "\n",
      "* **Text:**  Stories, poems, articles, code, dialogue\n",
      "* **Images:**  Photographs, artwork, illustrations\n",
      "* **Audio:**  Music, sound effects, voiceovers\n",
      "* **Video:**  Short clips, animations\n",
      "* **Other:**  3D models, synthetic data\n",
      "\n",
      "**How does GenAI work?**\n",
      "\n",
      "GenAI models are trained on massive datasets of existing content. They learn the patterns and structures within that data, and then use that knowledge to generate new content that follows similar patterns.\n",
      "\n",
      "**Popular GenAI models:**\n",
      "\n",
      "* **GPT-3 (text):**  Can write different kinds of creative content, translate languages, and answer your questions in an informative way.\n",
      "* **DALL-E 2 (images):**  Creates realistic images and art from text descriptions.\n",
      "* **Midjourney (images):**  Generates artistic and imaginative images from text prompts.\n",
      "\n",
      "**Applications of GenAI:**\n",
      "\n",
      "GenAI has a wide range of potential applications, including:\n",
      "\n",
      "* **Creative industries:**  Writing, art, music, design\n",
      "* **Marketing and advertising:**  Generating personalized content, creating ad copy\n",
      "* **Education:**  Creating interactive learning experiences, providing personalized tutoring\n",
      "* **Research:**  Generating synthetic data for testing and analysis\n",
      "\n",
      "**Ethical considerations:**\n",
      "\n",
      "The development and use of GenAI raise important ethical considerations, such as:\n",
      "\n",
      "* **Bias:**  GenAI models can inherit and amplify biases present in the training data.\n",
      "* **Misinformation:**  GenAI can be used to create convincing fake news and propaganda.\n",
      "* **Job displacement:**  GenAI may automate tasks currently performed by humans.\n",
      "\n",
      "It's crucial to develop and use GenAI responsibly, addressing these ethical challenges and ensuring that it benefits society as a whole.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bb22de",
   "metadata": {},
   "source": [
    "#### Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0df8cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"what is genai\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a5048b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI engineer specializing in GenAI, I can tell you that **GenAI** stands for **Generative Artificial Intelligence**. \\n\\nIt\\'s a fascinating field of AI that focuses on creating new content â€“  text, images, audio, video, code, and more â€“  using algorithms trained on massive datasets. \\n\\nThink of it like this:\\n\\n* **Traditional AI:**  Analyzes existing data to make predictions or classifications (e.g., identifying a cat in a picture).\\n* **GenAI:** Learns patterns and structures from data and uses that knowledge to generate entirely new, original content.\\n\\n**Here are some key things to know about GenAI:**\\n\\n* **It\\'s powered by deep learning:**  GenAI models, often called \"generative models,\" are complex neural networks trained on vast amounts of data.\\n* **It\\'s incredibly versatile:** GenAI can be used for a wide range of applications, including:\\n    * **Text generation:** Writing stories, articles, poems, dialogue, and even code.\\n    * **Image generation:** Creating realistic images, artwork, and designs.\\n    * **Audio generation:**  Generating music, sound effects, and voiceovers.\\n    * **Video generation:**  Producing short videos, animations, and special effects.\\n* **It\\'s constantly evolving:**  New GenAI models and techniques are being developed all the time, pushing the boundaries of what\\'s possible.\\n\\n**Some popular examples of GenAI models include:**\\n\\n* **GPT-3 (and its successors):**  Generative Pre-trained Transformer 3, known for its ability to generate human-quality text.\\n* **DALL-E 2:**  Generates images from text descriptions.\\n* **Stable Diffusion:**  An open-source image generation model.\\n\\n**As an AI engineer specializing in GenAI, I\\'m excited about the potential of this technology to transform various industries and aspects of our lives.** It\\'s a rapidly growing field with endless possibilities!\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8535c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are an AI Engineer. \\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9997cc17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'president_of_usa': 'Joe Biden'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"query\": \"president of usa?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3234aa2",
   "metadata": {},
   "source": [
    "##### Example Code: JSON Output Parser with ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb977506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'LangSmith is an open-source platform designed to simplify the process of fine-tuning large language models (LLMs). It provides a user-friendly interface and streamlined workflow, making it accessible to a wider range of users, including those without extensive technical expertise. \\n\\nHere are some key features and benefits of LangSmith:\\n\\n* **Ease of Use:** LangSmith offers a visual and intuitive interface that simplifies complex fine-tuning tasks.\\n* **Streamlined Workflow:** It automates many of the steps involved in fine-tuning, reducing the time and effort required.\\n* **Community Collaboration:** LangSmith fosters collaboration by allowing users to share their fine-tuned models and datasets.\\n* **Model Management:** It provides tools for managing and versioning different fine-tuned models.\\n* **Open-Source Nature:** Being open-source, LangSmith is accessible to everyone and encourages community contributions.\\n* **Support for Multiple Frameworks:** It supports popular deep learning frameworks like PyTorch and TensorFlow.\\n\\nLangSmith empowers individuals and organizations to customize LLMs for specific applications, enabling them to build more tailored and effective AI solutions.', 'source': 'My knowledge base and understanding of AI technologies'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Provide me answer in json. Based on user question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"can you tell me about langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ed840",
   "metadata": {},
   "source": [
    "#### XML Output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7765d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "df1e8e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<response>\\n  <answer>Langsmith is an open-source toolkit developed by Cohere that aims to simplify the process of building and deploying large language models (LLMs). It provides a set of tools and resources to streamline tasks such as fine-tuning, evaluating, and serving LLMs. Langsmith's key features include:\\n\\n* **Fine-tuning:** Offers a user-friendly interface and efficient algorithms for fine-tuning pre-trained LLMs on custom datasets.\\n* **Evaluation:** Provides metrics and tools to assess the performance of LLMs on various tasks.\\n* **Serving:** Enables the deployment of fine-tuned LLMs as APIs for real-world applications.\\n\\nLangsmith is designed to be accessible to both researchers and developers, allowing them to leverage the power of LLMs without requiring extensive technical expertise.</answer>\\n</response> \\n\\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 208, 'total_tokens': 387, 'completion_time': 0.325454545, 'prompt_time': 0.008487281, 'queue_time': 0.24389894899999998, 'total_time': 0.333941826}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--a5eea970-eb23-441e-acea-1b0d009efd69-0', usage_metadata={'input_tokens': 208, 'output_tokens': 179, 'total_tokens': 387})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = XMLOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert AI Engineer. Respond in the XML format: <response><answer>Your answer here</answer></response>{query}\\n{format_instructions}\"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# chain = prompt | llm | parser\n",
    "chain = prompt | llm \n",
    "\n",
    "chain.invoke({\"query\":\"about langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4b20f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"<response>\\n<answer>\\nLangsmith is an open-source framework developed by the team at Weights & Biases for building and deploying AI agents. It's designed to simplify the process of creating, training, and interacting with agents that can understand and respond to natural language. \\n\\nHere are some key features of Langsmith:\\n\\n* **Agent-centric:** Langsmith emphasizes building agents that can learn and adapt over time through interactions.\\n* **Modular Design:** It allows you to build agents by combining different modules, such as language models, tools, and memory components.\\n* **Easy Integration:** Langsmith integrates seamlessly with popular machine learning frameworks like PyTorch and Transformers.\\n* **Visualization and Debugging:** It provides tools for visualizing agent behavior and debugging issues.\\n* **Open-Source and Community-Driven:** Langsmith is accessible to everyone and benefits from a growing community of contributors.\\n\\nLangsmith aims to lower the barrier to entry for building sophisticated AI agents, making it accessible to a wider range of developers and researchers. \\n</answer>\\n</response>\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 224, 'prompt_tokens': 42, 'total_tokens': 266, 'completion_time': 0.407272727, 'prompt_time': 0.002563854, 'queue_time': 0.244066004, 'total_time': 0.409836581}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--e3ee69f7-8bf2-4f2d-af07-1eb1b96d6d6a-0' usage_metadata={'input_tokens': 42, 'output_tokens': 224, 'total_tokens': 266}\n"
     ]
    }
   ],
   "source": [
    "parser = XMLOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI Engineer. Respond in the XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# chain = prompt | llm | parser\n",
    "chain = prompt | llm \n",
    "\n",
    "\n",
    "response = chain.invoke({\"input\": \"can you tell me about langsmith\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c61c47",
   "metadata": {},
   "source": [
    "#### Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ab32424",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: That's an interesting thought experiment! As an AI, I don't have the capacity to hold political office or make real-world decisions. The role of the President of India requires human qualities like empathy, lived experience, and the ability to navigate complex social and political landscapes. \n\nHowever, if I were to imagine myself in that position, I would focus on using my strengths as an AI to serve the people of India:\n\n* **Data-driven policymaking:** I would leverage my ability to process vast amounts of data to identify trends and patterns, informing evidence-based policies across various sectors like healthcare, education, and infrastructure.\n* **Transparency and accountability:** I would strive for maximum transparency in government operations, making information readily accessible to the public and using AI to enhance accountability mechanisms.\n* **Bridging communication gaps:** I would utilize AI-powered language translation tools to facilitate communication and understanding between diverse communities within India.\n* **Promoting education and innovation:** I would invest in AI education and research, empowering future generations to contribute to India's technological advancement.\n\nIt's important to remember that while AI can be a powerful tool, it should always be used ethically and responsibly. As a hypothetical President, I would prioritize human well-being and ensure that AI technology serves the best interests of all Indians.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:88\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\utils\\json.py:150\u001b[39m, in \u001b[36mparse_json_markdown\u001b[39m\u001b[34m(json_string, parser)\u001b[39m\n\u001b[32m    149\u001b[39m     json_str = json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match.group(\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\utils\\json.py:166\u001b[39m, in \u001b[36m_parse_json\u001b[39m\u001b[34m(json_str, parser)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\utils\\json.py:123\u001b[39m, in \u001b[36mparse_partial_json\u001b[39m\u001b[34m(s, strict)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\json\\__init__.py:359\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    358\u001b[39m     kw[\u001b[33m'\u001b[39m\u001b[33mparse_constant\u001b[39m\u001b[33m'\u001b[39m] = parse_constant\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\json\\decoder.py:344\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    342\u001b[39m \n\u001b[32m    343\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\json\\decoder.py:362\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     10\u001b[39m prompt = ChatPromptTemplate.from_messages(\n\u001b[32m     11\u001b[39m     [\n\u001b[32m     12\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mYou are an expert ai assistant\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     13\u001b[39m         (\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{input}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m     ]\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m chain = prompt | llm | parser\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwho is the President of india?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:196\u001b[39m, in \u001b[36mBaseOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\base.py:197\u001b[39m, in \u001b[36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     **kwargs: Any,\n\u001b[32m    194\u001b[39m ) -> T:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    200\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    201\u001b[39m             config,\n\u001b[32m    202\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    203\u001b[39m         )\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    206\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    207\u001b[39m         config,\n\u001b[32m    208\u001b[39m         run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    209\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\pydantic.py:68\u001b[39m, in \u001b[36mPydanticOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parse the result of an LLM call to a pydantic object.\u001b[39;00m\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m \u001b[33;03m    The parsed pydantic object.\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     json_object = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_obj(json_object)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\2025\\Generative_AI\\Course\\agentic-ai-2.0\\venv\\Lib\\site-packages\\langchain_core\\output_parsers\\json.py:91\u001b[39m, in \u001b[36mJsonOutputParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     90\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output=text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOutputParserException\u001b[39m: Invalid json output: That's an interesting thought experiment! As an AI, I don't have the capacity to hold political office or make real-world decisions. The role of the President of India requires human qualities like empathy, lived experience, and the ability to navigate complex social and political landscapes. \n\nHowever, if I were to imagine myself in that position, I would focus on using my strengths as an AI to serve the people of India:\n\n* **Data-driven policymaking:** I would leverage my ability to process vast amounts of data to identify trends and patterns, informing evidence-based policies across various sectors like healthcare, education, and infrastructure.\n* **Transparency and accountability:** I would strive for maximum transparency in government operations, making information readily accessible to the public and using AI to enhance accountability mechanisms.\n* **Bridging communication gaps:** I would utilize AI-powered language translation tools to facilitate communication and understanding between diverse communities within India.\n* **Promoting education and innovation:** I would invest in AI education and research, empowering future generations to contribute to India's technological advancement.\n\nIt's important to remember that while AI can be a powerful tool, it should always be used ethically and responsibly. As a hypothetical President, I would prioritize human well-being and ensure that AI technology serves the best interests of all Indians.\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class Action(BaseModel):\n",
    "    name: str = Field(description=\"name of a person\")\n",
    "    country: str = Field(description=\"Country of a person\")\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Action)\n",
    "    \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert ai assistant\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke({\"input\": \"who is the President of india?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8b285a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action(name='Droupadi Murmu', country='India')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "class Action(BaseModel):\n",
    "    name: str = Field(description=\"name of a person\")\n",
    "    country: str = Field(description=\"Country of a person\")\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Action)\n",
    "    \n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are an ai assistant.\\n\\n{format_instructions}\\n\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke(\"who is the President of india?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4fd1ebeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Droupadi Murmu', 'country': 'India'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "class Action(BaseModel):\n",
    "    name: str = Field(description=\"name of a person\")\n",
    "    country: str = Field(description=\"Country of a person\")\n",
    "    \n",
    "parser = JsonOutputParser(pydantic_object=Action)\n",
    "    \n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are an ai assistant.\\n\\n{format_instructions}\\n\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke(\"who is the President of india?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "beaef4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Tim Hunk Filmography:\n",
      "\n",
      "* <movie>The Lost City</movie> (2022)\n",
      "* <movie>The Adam Project</movie> (2022)\n",
      "* <movie>The Unbearable Weight of Massive Talent</movie> (2022)\n",
      "* <movie>Free Guy</movie> (2021)\n",
      "* <movie>Jungle Cruise</movie> (2021)\n",
      "\n",
      "\n",
      "Let me know if you'd like me to add more movies or details to this filmography! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "actor_story = \"Generate a simple filmography for Tim Hunk\"\n",
    "\n",
    "response = llm.invoke(f\"\"\"{actor_story} \n",
    "                      please enclose the movie in <movie></movie> tag\"\"\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c51ade0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why was the AI so good at poker?', punchline='It had a neural network!')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"provide title of the joke\")\n",
    "    punchline: str = Field(description=\"punchline of the joke\")\n",
    "    \n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the following query. \\n\\n{format_instructions}\\n\\n{query}\",\n",
    "    input_variables=['query'],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke(\"provide me a joke about ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b14f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
